# Regression Test
--------------------------

How-to
Create a file in the directory of the slurm source code under slurm/testsuite/expect/globals.local
To contain the following.  This will overwrite the existing globals file with your specific directories.  To define site-specific state information, set the values in a file# named 'globals.local'. Those values will override any specified here.  Create the directory to point to your own source directory, your own build directory, where slurm is installed and where mpicc is installed. 

# for example:

$ cat globals.local

set slurm_dir  "/usr/local"

set build_dir  "/home/mine/SLURM/build_smd"

set src_dir    "/home/mine/SLURM/slurm.git"

set mpicc      "/usr/local/bin/mpicc"

Make sure that your slurm is running and the environment variables are pointing to the right slurm and issue the following command
nohup ./regression > <filename.log>  2>&1 &

For example

nohup ./regression >/app/slurm/nlk/regression_results/17.02-7.Bull.1.16.59.el7.log 2>&1 &

Once the tests have completed you can now open the log file from above, do a search on the word failures and see which tests failed.

The total of the tests is shown at the bottom of the file.  


# Slurmdbd Config
---------------------

mysql> create user 'slurm'@'localhost' identified by 'password';
Query OK, 0 rows affected (0.00 sec)
 
mysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost';
Query OK, 0 rows affected (0.00 sec)
 
mysql@snowflake:~$ mysql
Welcome to the MySQL monitor.Commands end with ; or \g.
Your MySQL connection id is 538
Server version: 5.0.51a-3ubuntu5.1 (Ubuntu)
 
Type 'help;' or '\h' for help. Type '\c' to clear the buffer.
 
mysql> create user 'slurm'@'localhost' identified by 'password';
Query OK, 0 rows affected (0.00 sec)
 
mysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost';
Query OK, 0 rows affected (0.00 sec)


mysql> create database slurm_acct_db;

/usr/local/<slurm-name>/bin/sacctmgr create cluster <clustername>

Launch
First time, on controller node:

Start mariadb
mysql -e "grant all on slurm_acct_db.* to 'root'@'localhost'"
mysql -e "grant all on slurm_acct_db.* to ''@'localhost'"
slurm_acct_db is defined in slurmdbd.conf
run /usr/local/<slurm-name>/sbin/slurmdbd
/usr/local/<slurm-name>/bin/sacctmgr create cluster <clustername>
run /usr/local/<slurm-name>/sbin/slurmctld
Other times, on controller node:

start mariadb
run  /usr/local/<slurm-name>/sbin/slurmctld and  /usr/local/<slurm-name>/sbin/slurmdbd
Without accounting, on controller node:

run  /usr/local/<slurm-name>/sbin/slurmctld
On compute nodes:

run /usr/local/<slurm-name>/sbin/slurmd
Since the slurm processes are not running under services, you will need to use kill or pkill to end them.

If you are diskless, save your configuration in your home.

# Singularity Details
--------------------------------

Sregistry and Srepo support, Go language in high level

https://singularityhub.github.io/sregistry/
https://singularity-hub.org/collections

Singularity recipie, Singularity Image repositories.

singularity 3 version	srun	sbatch	salloc	plugstack.conf		
						
--container-image=<image|sregistry-id>						
--container-bind=<bind-path(s)>						
--container-contain=yes						
--container-epilog=<epilog-exec>						
--container-pid=<pid-path>						
--container-workdir=<workdir-path>						
--sregistry-timeout=<seconds>						
--container-instance=<path>[:myinstance]						
--container-instance=<myinstance>						
						
default_image=<image|sregistry-id|instance://myinstance>						
default_bind_path=<bind-path(s)>						
default_contain=yes						
default_epilog_path=<epilog-exec>						
default_overlay_path=<overlay-path>						
sregistry_path=<sregistry-images-path>						
default_pid_path=<pid-path>						
default_workdir_path=<workdir-path>						
default_sregistry_timeout=<seconds>						
						
singularity 2.5.2 containers framework	srun	sbatch	salloc	plugstack.conf		
						
--singularity-image=<image|sregistry-id>						
--singularity-bind=<bind-path(s)>						
--singularity-contain=yes						
--singularity-epilog=<epilog-exec>						
--singularity-pid=<pid-path>						
--singularity-workdir=<workdir-path>						
--sregistry-timeout=<seconds>						
						
default_image=<image|sregistry-id>						
default_bind_path=<bind-path(s)>						
default_contain=yes						
default_epilog_path=<epilog-exec>						
default_overlay_path=<overlay-path>						
sregistry_path=<sregistry-images-path>						
default_pid_path=<pid-path>						
default_workdir_path=<workdir-path>						
default_sregistry_timeout=<seconds>						
						
singularity prototype	srun	sbatch	salloc	plugstack.conf	sg_spank.conf	shortcut
						
--sgdebug						
--singularity-image=<image>						--sgimage
--singularity-env=<env_script>						--sgenv
						
install_dir=<install-dir-path>						
swrepo_sync_dir=<swrepo-dir-path>						


# GHC topology
-------------------

1) Hypercube topology is supported in slurm already
2) Generic Hypercube to support all Hypercube (n dimension)
3) Resourec Selection
	- Assign gray code or coordinates
	- Order coordinate by hilbert curve to covert n dimension to 1 dimension
	- Select nodes out of the hilbert curve
3) Flat tree inside the cell and GHC topology for inter cell
4) Dragonfly and Dragonfly+ topology support

# PAM and PAM adopt plugin
----------------------------
1) Check job and reservation details in pam plugin
2) pam_adopt with cgroup


# Netowrk topology benchmark
------------------------------
1) HP2P benchmark
https://github.com/cea-hpc/hp2p

2) Linktest benchmark

Avoid random test to get better results

# Compute benchmark
-------------------------

1) HPL - HPlinpack
2) Intel MPI benchmark
3) Miniapp (cloverleaf, etc) to reprsent domain

# Slurmtop

# PMI2 and PMIx support

# d-mod-k aware routing

# MPMD programming with gromacs_interactive
https://bitbucket.org/matthieu_dreher/gromacs_interactive // Matthieu Drehher

# MPI with FTI programming interface to support different checkpoint restart 

# SAM, Singularity pack, Spack and Easybuild support

# SOLID principle in the Python OOP programming
